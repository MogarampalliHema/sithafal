{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MogarampalliHema/sithafal/blob/main/chat_to_pdftask1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18527c4f",
      "metadata": {
        "id": "18527c4f",
        "outputId": "5f663e31-1f55-47fb-dcde-a3c28606c8aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -q -r ./requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b748fd20",
      "metadata": {
        "id": "b748fd20",
        "outputId": "b9da04fd-de3f-4cae-d96e-ce1ca02c5040"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07900179",
      "metadata": {
        "id": "07900179"
      },
      "source": [
        "### Loading Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ae19b5",
      "metadata": {
        "id": "e1ae19b5"
      },
      "outputs": [],
      "source": [
        "# loading PDF, DOCX and TXT files as LangChain Documents\n",
        "def load_document(file):\n",
        "    import os\n",
        "    name, extension = os.path.splitext(file)\n",
        "\n",
        "    if extension == '.pdf':\n",
        "        from langchain.document_loaders import PyPDFLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = PyPDFLoader(file)\n",
        "    elif extension == '.docx':\n",
        "        from langchain.document_loaders import Docx2txtLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = Docx2txtLoader(file)\n",
        "    elif extension == '.txt':\n",
        "        from langchain.document_loaders import TextLoader\n",
        "        loader = TextLoader(file)\n",
        "    else:\n",
        "        print('Document format is not supported!')\n",
        "        return None\n",
        "\n",
        "    data = loader.load()\n",
        "    return data\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38eb372c",
      "metadata": {
        "id": "38eb372c"
      },
      "outputs": [],
      "source": [
        "# wikipedia\n",
        "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
        "    from langchain.document_loaders import WikipediaLoader\n",
        "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
        "    data = loader.load()\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "367bb8ad",
      "metadata": {
        "id": "367bb8ad"
      },
      "source": [
        "### Chunking Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62c3cb39",
      "metadata": {
        "id": "62c3cb39"
      },
      "outputs": [],
      "source": [
        "def chunk_data(data, chunk_size=256):\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "    return chunks\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b298177",
      "metadata": {
        "id": "2b298177"
      },
      "source": [
        "### Calculating Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add191c6",
      "metadata": {
        "id": "add191c6"
      },
      "outputs": [],
      "source": [
        "def print_embedding_cost(texts):\n",
        "    import tiktoken\n",
        "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
        "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
        "    # check prices here: https://openai.com/pricing\n",
        "    print(f'Total Tokens: {total_tokens}')\n",
        "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27f92afa",
      "metadata": {
        "id": "27f92afa"
      },
      "source": [
        "### Embedding and Uploading to a Vector Database (Pinecone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c72c3cf",
      "metadata": {
        "id": "0c72c3cf"
      },
      "outputs": [],
      "source": [
        "def insert_or_fetch_embeddings(index_name, chunks):\n",
        "    # importing the necessary libraries and initializing the Pinecone client\n",
        "    import pinecone\n",
        "    from langchain_community.vectorstores import Pinecone\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    from pinecone import ServerlessSpec\n",
        "\n",
        "\n",
        "    pc = pinecone.Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
        "\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536, api_key=os.environ.get(\"OPEN_AI_KEY\"))  # 512 works as well\n",
        "\n",
        "    # loading from existing index\n",
        "    if index_name in pc.list_indexes().names():\n",
        "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
        "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        # creating the index and embedding the chunks into the index\n",
        "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
        "\n",
        "        # creating a new index\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1536,\n",
        "            metric='cosine',\n",
        "            spec=ServerlessSpec(\n",
        "                cloud=\"aws\",\n",
        "                region=\"us-east-1\"\n",
        "        )\n",
        "        )\n",
        "\n",
        "        # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
        "        # inserting the embeddings into the index and returning a new Pinecone vector store object.\n",
        "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
        "        print('Ok')\n",
        "\n",
        "    return vector_store\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ef1091",
      "metadata": {
        "id": "52ef1091"
      },
      "outputs": [],
      "source": [
        "def delete_pinecone_index(index_name='all'):\n",
        "    import pinecone\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    if index_name == 'all':\n",
        "        indexes = pc.list_indexes().names()\n",
        "        print('Deleting all indexes ... ')\n",
        "        for index in indexes:\n",
        "            pc.delete_index(index)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        print(f'Deleting index {index_name} ...', end='')\n",
        "        pc.delete_index(index_name)\n",
        "        print('Ok')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "462a94cb",
      "metadata": {
        "id": "462a94cb"
      },
      "source": [
        "### Asking and Getting Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b0a508",
      "metadata": {
        "id": "08b0a508"
      },
      "outputs": [],
      "source": [
        "def ask_and_get_answer(vector_store, q, k=3):\n",
        "    from langchain.chains import RetrievalQA\n",
        "    from langchain_openai import ChatOpenAI\n",
        "\n",
        "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1, api_key=os.environ.get(\"OPEN_AI_KEY\"))\n",
        "\n",
        "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
        "\n",
        "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "    answer = chain.invoke(q)\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1efe8fc",
      "metadata": {
        "id": "c1efe8fc"
      },
      "source": [
        "### Running Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0da518dc",
      "metadata": {
        "id": "0da518dc"
      },
      "outputs": [],
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ad0e41f-d82b-462d-8d51-f5cc83d836c5",
      "metadata": {
        "id": "8ad0e41f-d82b-462d-8d51-f5cc83d836c5"
      },
      "source": [
        "#### Ask a PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23fd7147",
      "metadata": {
        "id": "23fd7147",
        "outputId": "aecca5d0-9a2f-4576-8ff7-19acd53f8f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ./us_constitution.pdf\n",
            "You have 41 pages in your data\n",
            "There are 1174 characters in the page\n"
          ]
        }
      ],
      "source": [
        "data = load_document('./us_constitution.pdf')\n",
        "# print(data[1].page_content)\n",
        "# print(data[10].metadata)\n",
        "\n",
        "print(f'You have {len(data)} pages in your data')\n",
        "print(f'There are {len(data[20].page_content)} characters in the page')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11b7652",
      "metadata": {
        "id": "d11b7652"
      },
      "outputs": [],
      "source": [
        "# data = load_document('files/the_great_gatsby.docx')\n",
        "# print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6054462e",
      "metadata": {
        "id": "6054462e"
      },
      "outputs": [],
      "source": [
        "# data = load_from_wikipedia('GPT-4', 'de')\n",
        "# print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b5d7f5",
      "metadata": {
        "id": "64b5d7f5",
        "outputId": "646ba4fb-a159-4921-a455-2aaa0b02c723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "224\n"
          ]
        }
      ],
      "source": [
        "chunks = chunk_data(data)\n",
        "print(len(chunks))\n",
        "# print(chunks[10].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faa75175",
      "metadata": {
        "id": "faa75175",
        "outputId": "c18465cb-afc3-48f1-e045-a3827cc701d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Tokens: 9842\n",
            "Embedding Cost in USD: 0.000197\n"
          ]
        }
      ],
      "source": [
        "print_embedding_cost(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f4f798d",
      "metadata": {
        "id": "8f4f798d",
        "outputId": "e0d40023-f8c9-4a06-da03-cbdc2753a7e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleting all indexes ... \n",
            "Ok\n"
          ]
        }
      ],
      "source": [
        "delete_pinecone_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8525a620",
      "metadata": {
        "id": "8525a620",
        "outputId": "8d46c93a-eef6-44d9-81aa-f6a785e09f49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index askadocument already exists. Loading embeddings ... Ok\n"
          ]
        }
      ],
      "source": [
        "index_name = 'askadocument'\n",
        "# vector_store = insert_or_fetch_embeddings(index_name=index_name, chunks=chunks)\n",
        "vector_store = insert_or_fetch_embeddings(index_name=index_name, chunks=chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b409649f-cc34-4de6-83cb-3906ad046fdf",
      "metadata": {
        "id": "b409649f-cc34-4de6-83cb-3906ad046fdf"
      },
      "source": [
        "#### While Loop for Asking Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ddb48f",
      "metadata": {
        "id": "00ddb48f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "i = 1\n",
        "print('Write Quit or Exit to quit.')\n",
        "while True:\n",
        "    q = input(f'Question #{i}: ')\n",
        "    i = i + 1\n",
        "    if q.lower() in ['quit', 'exit']:\n",
        "        print('Quitting ... bye bye!')\n",
        "        time.sleep(2)\n",
        "        break\n",
        "\n",
        "    answer = ask_and_get_answer(vector_store, q)\n",
        "    print(f'\\nAnswer: {answer}')\n",
        "    print(f'\\n {\"-\" * 50} \\n')\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb28d1c5-1abe-456b-9ccb-0f179795ab3e",
      "metadata": {
        "id": "fb28d1c5-1abe-456b-9ccb-0f179795ab3e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pdfplumber pytesseract pillow pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiMSONHuSSAu",
        "outputId": "66762d57-9369-4d0d-bfba-dd0136f1b276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        }
      ],
      "id": "fiMSONHuSSAu"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNeP55SDJ6gI"
      },
      "id": "tNeP55SDJ6gI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to clean and normalize text\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean extracted text by removing excessive spaces, newlines, and invalid symbols.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'\\n+', ' ', text)  # Replace newlines\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII\n",
        "    text = re.sub(r'[^\\w\\s\\.\\%\\:\\$\\-]', '', text)  # Keep valid symbols\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n",
        "    return text\n",
        "\n",
        "# Function to preprocess images for OCR\n",
        "def preprocess_image_for_ocr(image):\n",
        "    \"\"\"\n",
        "    Preprocess image: grayscale, threshold, contrast enhancement, and resizing.\n",
        "    \"\"\"\n",
        "    image = image.convert(\"L\")  # Grayscale\n",
        "    image = ImageEnhance.Contrast(image).enhance(2.5)  # Increase contrast\n",
        "    image = ImageOps.invert(image)  # Invert for better text visibility\n",
        "    image = image.resize((image.width * 2, image.height * 2), Image.Resampling.LANCZOS)  # Resize\n",
        "    return image\n",
        "\n",
        "# OCR function with fallback to different PSM modes\n",
        "def ocr_page(pdf_path, page_number):\n",
        "    \"\"\"\n",
        "    Perform OCR with preprocessing and adaptive PSM modes.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            page = pdf.pages[page_number]\n",
        "            image = page.to_image(resolution=300).original\n",
        "            image = preprocess_image_for_ocr(image)\n",
        "\n",
        "            # First OCR attempt with PSM 6\n",
        "            text = pytesseract.image_to_string(image, config=\"--psm 6\")\n",
        "            if not text.strip():\n",
        "                # Fallback to PSM 4 for better layout parsing\n",
        "                text = pytesseract.image_to_string(image, config=\"--psm 4\")\n",
        "            print(f\"OCR performed on Page {page_number + 1}\")\n",
        "            return clean_text(text)\n",
        "    except Exception as e:\n",
        "        return f\"Error performing OCR on Page {page_number + 1}: {e}\"\n",
        "\n",
        "# Function to dynamically format and align OCR output\n",
        "def format_ocr_output(text):\n",
        "    \"\"\"\n",
        "    Dynamically format OCR output into structured lines with labels and values.\n",
        "    \"\"\"\n",
        "    # Patterns for degrees, percentages, and monetary values\n",
        "    degree_pattern = r'(Doctoral|Professional|Masters|Bachelors|Associates|Some college|High school|Less than high school)\\s(degree|diploma)'\n",
        "    value_pattern = r'(\\d+\\.\\d+\\%|\\$\\d+|\\d+)'\n",
        "\n",
        "    # Find all degrees and values dynamically\n",
        "    degrees = re.findall(degree_pattern, text)\n",
        "    values = re.findall(value_pattern, text)\n",
        "\n",
        "    formatted_output = []\n",
        "    idx = 0\n",
        "\n",
        "    # Align degrees with their respective values\n",
        "    for degree in degrees:\n",
        "        degree_text = \" \".join(degree)\n",
        "        value = values[idx] if idx < len(values) else \"N/A\"\n",
        "        formatted_output.append(f\"{degree_text}: {value}\")\n",
        "        idx += 1\n",
        "\n",
        "    # Print formatted lines\n",
        "    for line in formatted_output:\n",
        "        print(line)\n",
        "\n",
        "# Function to extract text from specific pages\n",
        "def extract_page_text(pdf_path, page_numbers):\n",
        "    \"\"\"\n",
        "    Extract text from specific pages using OCR as a fallback.\n",
        "    \"\"\"\n",
        "    page_data = {}\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_num in page_numbers:\n",
        "            try:\n",
        "                page = pdf.pages[page_num]\n",
        "                text = page.extract_text()\n",
        "                if text and len(text.strip()) > 0:\n",
        "                    page_data[f\"Page {page_num+1}\"] = clean_text(text)\n",
        "                else:\n",
        "                    print(f\"No text found on Page {page_num+1}, performing OCR...\")\n",
        "                    page_data[f\"Page {page_num+1}\"] = ocr_page(pdf_path, page_num)\n",
        "            except IndexError:\n",
        "                page_data[f\"Page {page_num+1}\"] = \"Page number out of range.\"\n",
        "    return page_data\n",
        "\n",
        "# Function to extract tabular data\n",
        "def extract_table_data(pdf_path, page_number):\n",
        "    \"\"\"\n",
        "    Extract and clean table data from a specific page.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            page = pdf.pages[page_number]\n",
        "            tables = page.extract_tables()\n",
        "            if tables:\n",
        "                cleaned_table = [[re.sub(r'\\s+', ' ', str(cell).strip()) for cell in row] for row in tables[0]]\n",
        "                df = pd.DataFrame(cleaned_table[1:], columns=cleaned_table[0])\n",
        "                print(f\"Table extracted successfully from Page {page_number + 1}\")\n",
        "                return df\n",
        "            else:\n",
        "                print(f\"No tables found on Page {page_number + 1}\")\n",
        "                return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting table from Page {page_number + 1}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# PDF File Path\n",
        "pdf_path = \"sample.pdf\"\n",
        "\n",
        "# Extract text and format Page 2\n",
        "print(\"----- Page 2: Unemployment Information by Degree -----\")\n",
        "specific_pages = extract_page_text(pdf_path, [1, 5])\n",
        "page_2_text = specific_pages.get(\"Page 2\", \"No data found.\")\n",
        "format_ocr_output(page_2_text)\n",
        "\n",
        "# Extract and display tabular data from Page 6\n",
        "print(\"\\n----- Page 6: Tabular Data -----\")\n",
        "page_6_table = extract_table_data(pdf_path, 5)\n",
        "if not page_6_table.empty:\n",
        "    print(\"Extracted Tabular Data:\")\n",
        "    print(page_6_table)\n",
        "else:\n",
        "    print(\"No tabular data found on Page 6.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rydYJ_bXQHMa",
        "outputId": "568b3029-fe81-4faf-a4dd-89c26f6194db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Page 2: Unemployment Information by Degree -----\n",
            "No text found on Page 2, performing OCR...\n",
            "OCR performed on Page 2\n",
            "Professional degree: 2013\n",
            "Masters degree: 2013\n",
            "Bachelors degree: 2\n",
            "Associates degree: 4\n",
            "High school diploma: 5\n",
            "\n",
            "----- Page 6: Tabular Data -----\n",
            "Table extracted successfully from Page 6\n",
            "Extracted Tabular Data:\n",
            "                                                Year      2010      2011  \\\n",
            "0                                     All Industries  26093515  27535971   \n",
            "1                                      Manufacturing   4992521   5581942   \n",
            "2   Finance, Insurance, Real Estate, Rental, Leasing   4522451   4618678   \n",
            "3  Arts, Entertainment, Recreation, Accommodation...    964032   1015238   \n",
            "4                                              Other  15614511  16320113   \n",
            "\n",
            "       2012      2013      2014      2015  \n",
            "0  28663246  29601191  30895407  31397023  \n",
            "1   5841608   5953299   6047477   5829554  \n",
            "2   4797313   5031881   5339678   5597018  \n",
            "3   1076249   1120496   1189646   1283813  \n",
            "4  16948076  17495515  18318606  18686638  \n"
          ]
        }
      ],
      "id": "rydYJ_bXQHMa"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}